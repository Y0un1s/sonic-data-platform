{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%pip install annoy"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "spark_pool": null,
       "statement_id": 8,
       "statement_ids": [
        4,
        5,
        6,
        7,
        8
       ],
       "state": "finished",
       "livy_statement_state": "available",
       "session_id": "18184d11-bac4-4d8a-a7b4-898a7859c4e4",
       "normalized_state": "finished",
       "queued_time": "2025-11-25T12:35:04.844565Z",
       "session_start_time": null,
       "execution_start_time": "2025-11-25T12:35:04.8449895Z",
       "execution_finish_time": "2025-11-25T12:35:40.4854324Z",
       "parent_msg_id": "0de1f314-ec74-49cb-8dda-8482ed1a85a4"
      },
      "text/plain": "StatementMeta(, 18184d11-bac4-4d8a-a7b4-898a7859c4e4, 8, Finished, Available, Finished)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting annoy\n  Downloading annoy-1.17.3.tar.gz (647 kB)\n\u001B[2K     \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m647.5/647.5 kB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \bdone\n\u001B[?25hBuilding wheels for collected packages: annoy\n  Building wheel for annoy (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n\u001B[?25h  Created wheel for annoy: filename=annoy-1.17.3-cp311-cp311-linux_x86_64.whl size=77476 sha256=244679487111355cc08c5559cc9c6457f1397ff6905fdf84dd981c6fc41ca34b\n  Stored in directory: /home/trusted-service-user/.cache/pip/wheels/33/e5/58/0a3e34b92bedf09b4c57e37a63ff395ade6f6c1099ba59877c\nSuccessfully built annoy\nInstalling collected packages: annoy\nSuccessfully installed annoy-1.17.3\n\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython -m pip install --upgrade pip\u001B[0m\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"
     ]
    }
   ],
   "execution_count": 2,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "afb9af4e-dff8-4bee-a98c-08a19e6d39af"
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 1 ‚Äî IMPORTS & LOAD MODEL\n",
    "# ============================================\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "# Load catalog + model + tables\n",
    "CATALOG_TABLE = \"gold_ml_catalog\"\n",
    "USER_EMB_TABLE = \"gold_two_tower_user_emb\"\n",
    "ITEM_EMB_TABLE = \"gold_two_tower_item_emb\"\n",
    "GOLD_LABEL_TABLE = \"gold_ml_training_set\"\n",
    "\n",
    "catalog_enriched = spark.table(CATALOG_TABLE)\n",
    "\n",
    "lgbm_model = joblib.load(\"/lakehouse/default/Files/models/lgbm_re_ranker.pkl\")\n",
    "\n",
    "print(\"‚úÖ Model & catalog loaded.\")\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "spark_pool": null,
       "statement_id": 10,
       "statement_ids": [
        10
       ],
       "state": "finished",
       "livy_statement_state": "available",
       "session_id": "18184d11-bac4-4d8a-a7b4-898a7859c4e4",
       "normalized_state": "finished",
       "queued_time": "2025-11-25T12:35:08.3138373Z",
       "session_start_time": null,
       "execution_start_time": "2025-11-25T12:35:46.9570679Z",
       "execution_finish_time": "2025-11-25T12:36:28.0142628Z",
       "parent_msg_id": "64b6892f-e1a5-4a4f-aa67-8cc7cf946fe7"
      },
      "text/plain": "StatementMeta(, 18184d11-bac4-4d8a-a7b4-898a7859c4e4, 10, Finished, Available, Finished)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Model & catalog loaded.\n"
     ]
    }
   ],
   "execution_count": 3,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "c4bd1f00-cf61-42ea-a1d2-586a3691ab0e"
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 2 ‚Äî UNIFIED DISCOVERY (LightGBM scoring)\n",
    "# Using PREBUILT Annoy Indexes\n",
    "# ============================================\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Tables\n",
    "ITEM_EMB_TABLE  = \"gold_two_tower_item_emb\"\n",
    "USER_EMB_TABLE  = \"gold_two_tower_user_emb\"\n",
    "GOLD_LABEL_TABLE = \"gold_ml_training_set\"\n",
    "\n",
    "# Arabic filter switch\n",
    "AVOID_ARABIC = True\n",
    "arabic_pattern = \"[\\\\u0600-\\\\u06FF]\"\n",
    "\n",
    "# =====================================================\n",
    "# LOAD PREBUILT ANNOY INDEXES (Friends + Ocean)\n",
    "# =====================================================\n",
    "\n",
    "dim = 256  # embedding size\n",
    "\n",
    "# ---- Friends index ----\n",
    "friends_index = AnnoyIndex(dim, 'angular')\n",
    "friends_index.load(\"/lakehouse/default/Files/annoy/friends_index.ann\")\n",
    "friends_map = joblib.load(\"/lakehouse/default/Files/annoy/friends_map.pkl\")\n",
    "\n",
    "# ---- Ocean index ----\n",
    "ocean_index = AnnoyIndex(dim, 'angular')\n",
    "ocean_index.load(\"/lakehouse/default/Files/annoy/ocean_index.ann\")\n",
    "ocean_map = joblib.load(\"/lakehouse/default/Files/annoy/ocean_map.pkl\")\n",
    "\n",
    "print(\"‚úÖ Prebuilt Annoy indexes loaded.\")\n",
    "\n",
    "\n",
    "# unified_discovery function\n",
    "def unified_discovery(\n",
    "    target_user_id: str,\n",
    "    ocean_weight: float = 0.2,\n",
    "    base_k_min: int = 200,\n",
    "    base_k_max: int = 1000,\n",
    "    top_n: int = 30\n",
    "):\n",
    "    print(f\"\\nüéØ Unified Discovery for user: {target_user_id}\")\n",
    "    print(f\"üåä Using ocean_weight = {ocean_weight}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1) Load user embedding\n",
    "    # --------------------------------------------------------\n",
    "    user_row = (\n",
    "        spark.table(USER_EMB_TABLE)\n",
    "        .filter(col(\"spotify_user_id\") == target_user_id)\n",
    "        .limit(1)\n",
    "        .collect()\n",
    "    )\n",
    "    if not user_row:\n",
    "        print(\"‚ö† User not found in user embedding table.\")\n",
    "        return None\n",
    "\n",
    "    user_vec = np.array(user_row[0][\"vector\"], dtype=\"float32\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Block known tracks (positives)\n",
    "    # --------------------------------------------------------\n",
    "    history_rows = (\n",
    "        spark.table(GOLD_LABEL_TABLE)\n",
    "        .filter(col(\"spotify_user_id\") == target_user_id)\n",
    "        .filter(col(\"label\") == 1)\n",
    "        .select(\"spotify_id\")\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "    history_set = {r.spotify_id for r in history_rows}\n",
    "    print(f\"üö´ Blocking {len(history_set)} known tracks.\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3) Retrieve using prebuilt ANN indexes\n",
    "    # --------------------------------------------------------\n",
    "    activity = max(1, len(history_set))\n",
    "    k = int(np.clip(activity * 10, base_k_min, base_k_max))\n",
    "    print(f\"üìå Retrieval size k = {k}\")\n",
    "\n",
    "    ocean_weight = float(np.clip(ocean_weight, 0.0, 1.0))\n",
    "    k_ocean   = int(k * ocean_weight)\n",
    "    k_friends = k - k_ocean\n",
    "\n",
    "    print(f\"üîÑ Retrieval split: {k_ocean} OCEAN + {k_friends} FRIENDS\")\n",
    "\n",
    "    # ---- FRIENDS retrieval ----\n",
    "    friend_ids = (\n",
    "        friends_index.get_nns_by_vector(user_vec, k_friends)\n",
    "        if k_friends > 0 else []\n",
    "    )\n",
    "    friend_candidates = [friends_map[i] for i in friend_ids]\n",
    "\n",
    "    # ---- OCEAN retrieval ----\n",
    "    ocean_ids = (\n",
    "        ocean_index.get_nns_by_vector(user_vec, k_ocean)\n",
    "        if k_ocean > 0 else []\n",
    "    )\n",
    "    ocean_candidates = [ocean_map[i] for i in ocean_ids]\n",
    "\n",
    "    # Merge + remove duplicates + remove known tracks\n",
    "    candidates = list({\n",
    "        cid for cid in (friend_candidates + ocean_candidates)\n",
    "        if cid not in history_set\n",
    "    })\n",
    "\n",
    "    print(f\"üåä OCEAN used:   {len(ocean_candidates)}\")\n",
    "    print(f\"ü§ù FRIENDS used: {len(friend_candidates)}\")\n",
    "    print(f\"‚úÖ Total unseen candidates: {len(candidates)}\")\n",
    "\n",
    "    if not candidates:\n",
    "        print(\"‚ö† No unseen candidates found.\")\n",
    "        return None\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4) Join candidates with catalog (audio features)\n",
    "    # --------------------------------------------------------\n",
    "    candidates_df = spark.createDataFrame(\n",
    "        [(target_user_id, sid) for sid in candidates],\n",
    "        [\"spotify_user_id\", \"spotify_id\"]\n",
    "    )\n",
    "\n",
    "    recs = candidates_df.join(catalog_enriched, \"spotify_id\", \"left\")\n",
    "\n",
    "    if AVOID_ARABIC:\n",
    "        recs = recs.filter(~col(\"track_name\").rlike(arabic_pattern))\n",
    "\n",
    "    # Fill missing audio features\n",
    "    recs = recs.fillna({c: 0.0 for c in AUDIO_FEATURES})\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5) SCORE with LightGBM\n",
    "    # --------------------------------------------------------\n",
    "    pdf = recs.select(\n",
    "        \"spotify_id\",\n",
    "        \"track_name\",\n",
    "        \"artist_name\",\n",
    "        *AUDIO_FEATURES\n",
    "    ).toPandas()\n",
    "\n",
    "    if pdf.empty:\n",
    "        print(\"‚ö† No rows to score after filtering.\")\n",
    "        return None\n",
    "\n",
    "    X = pdf[AUDIO_FEATURES].values.astype(\"float32\")\n",
    "    scores = lgbm_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    pdf[\"score\"] = scores\n",
    "\n",
    "    preds = spark.createDataFrame(pdf)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 6) Rank + dedupe\n",
    "    # --------------------------------------------------------\n",
    "    w = Window.partitionBy(\"track_name\", \"artist_name\").orderBy(F.desc(\"score\"))\n",
    "\n",
    "    final_df = (\n",
    "        preds.withColumn(\"rk\", F.row_number().over(w))\n",
    "             .filter(col(\"rk\") == 1)\n",
    "             .orderBy(F.desc(\"score\"))\n",
    "             .limit(top_n)\n",
    "             .select(\n",
    "                 \"track_name\",\n",
    "                 \"artist_name\",\n",
    "                 \"score\",\n",
    "                 F.round(\"tempo\", 1).alias(\"tempo\"),\n",
    "                 F.round(\"energy\", 2).alias(\"energy\"),\n",
    "                 F.round(\"danceability\", 2).alias(\"dance\"),\n",
    "                 F.round(\"valence\", 2).alias(\"mood\"),\n",
    "                 F.round(\"loudness\", 1).alias(\"loud\"),\n",
    "                 F.round(\"acousticness\", 2).alias(\"acoust\"),\n",
    "             )\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüéß Top {top_n} Recommendations:\")\n",
    "    final_df.show(truncate=False)\n",
    "\n",
    "    return final_df\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "spark_pool": null,
       "statement_id": 11,
       "statement_ids": [
        11
       ],
       "state": "finished",
       "livy_statement_state": "available",
       "session_id": "18184d11-bac4-4d8a-a7b4-898a7859c4e4",
       "normalized_state": "finished",
       "queued_time": "2025-11-25T12:35:09.7679562Z",
       "session_start_time": null,
       "execution_start_time": "2025-11-25T12:36:28.0167521Z",
       "execution_finish_time": "2025-11-25T12:36:34.7296028Z",
       "parent_msg_id": "e8ed7263-fa92-4006-a799-afb66355e529"
      },
      "text/plain": "StatementMeta(, 18184d11-bac4-4d8a-a7b4-898a7859c4e4, 11, Finished, Available, Finished)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Prebuilt Annoy indexes loaded.\n"
     ]
    }
   ],
   "execution_count": 4,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "1da23e57-6a84-4474-aa70-8331304e4eea"
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# CELL 3 ‚Äî BATCH RECOMMENDER JOB (PRODUCTION)\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "USER_TABLE   = \"silver_user_profile\"\n",
    "OUTPUT_TABLE = \"gold_recommendations\"\n",
    "TOP_N        = 15\n",
    "\n",
    "# Load all users with emails + display names\n",
    "user_df = (\n",
    "    spark.table(USER_TABLE)\n",
    "    .select(\"spotify_user_id\", \"email\", \"display_name\")\n",
    "    .dropna(subset=[\"spotify_user_id\"])\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# LOAD MODEL METADATA\n",
    "# ============================================\n",
    "\n",
    "meta = spark.table(\"gold_model_metadata\").orderBy(F.desc(\"trained_at\")).limit(1).collect()[0]\n",
    "\n",
    "MODEL_VERSION = meta[\"model_version\"]\n",
    "MODEL_TRAINED_AT = meta[\"trained_at\"]\n",
    "\n",
    "print(\"üìå Loaded model metadata:\")\n",
    "print(\"MODEL_VERSION   =\", MODEL_VERSION)\n",
    "print(\"MODEL_TRAINED_AT=\", MODEL_TRAINED_AT)\n",
    "\n",
    "\n",
    "users = user_df.collect()\n",
    "print(f\"üë• Users to process: {len(users):,}\")\n",
    "\n",
    "batch_rows = []\n",
    "\n",
    "for user in users:\n",
    "    uid = user.spotify_user_id\n",
    "    email = user.email\n",
    "    display = user.display_name\n",
    "\n",
    "    print(f\"\\n‚û° Recommending for: {uid} ({display})\")\n",
    "\n",
    "    try:\n",
    "        recs = unified_discovery(uid, ocean_weight=0.2, top_n=TOP_N)\n",
    "        if recs is None:\n",
    "            continue\n",
    "\n",
    "        pdf = recs.toPandas()\n",
    "\n",
    "        for r in pdf.itertuples():\n",
    "            batch_rows.append((\n",
    "                uid,\n",
    "                email,\n",
    "                display,\n",
    "                r.track_name,\n",
    "                r.artist_name,\n",
    "                float(r.score),\n",
    "                MODEL_VERSION,\n",
    "                MODEL_TRAINED_AT\n",
    "            ))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error for {uid}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "# Safety\n",
    "if not batch_rows:\n",
    "    raise Exception(\"‚ùå No recommendations generated. batch_rows is empty!\")\n",
    "\n",
    "# Convert to Spark DF\n",
    "final_df = spark.createDataFrame(\n",
    "    batch_rows,\n",
    "    [\n",
    "        \"spotify_user_id\",\n",
    "        \"email\",\n",
    "        \"display_name\",\n",
    "        \"track_name\",\n",
    "        \"artist_name\",\n",
    "        \"score\",\n",
    "        \"model_version\",\n",
    "        \"model_trained_at\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add batch timestamp\n",
    "final_df = final_df.withColumn(\"batch_generated_at\", current_timestamp())\n",
    "\n",
    "# Save to Delta table\n",
    "final_df.write.mode(\"overwrite\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "print(f\"‚úÖ Saved recommendations to {OUTPUT_TABLE}\")\n",
    "print(f\"üì¶ Rows: {final_df.count():,}\")\n"
   ],
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"f33d3f6d-b1cb-4368-b6c2-cc81a87c92fb\",\"activityId\":\"18184d11-bac4-4d8a-a7b4-898a7859c4e4\",\"applicationId\":\"application_1764073549539_0001\",\"jobGroupId\":\"19\",\"advices\":{\"warn\":7}}"
    }
   },
   "id": "ece49018-e6da-4c84-9b20-621258cc2f33",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "dependencies": {
   "lakehouse": {
    "known_lakehouses": [
     {
      "id": "e9de4b82-79eb-4248-b4c5-bc19ae7fc9c2"
     }
    ],
    "default_lakehouse": "e9de4b82-79eb-4248-b4c5-bc19ae7fc9c2",
    "default_lakehouse_name": "sonic_lakehouse",
    "default_lakehouse_workspace_id": "93da16a5-3c33-440e-bbec-66866bce621b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
